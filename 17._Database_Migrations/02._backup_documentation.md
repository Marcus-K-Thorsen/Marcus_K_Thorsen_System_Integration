<div class="title-card">
    <h1>Backup Documentation</h1>
</div>

---

# What is backup documentation?

Backup documentation refers to the process of creating and maintaining records of how to back up and restore a database. It ensures that critical data can be recovered in case of data loss, corruption, or system failure. Proper backup documentation provides clear instructions for creating backups, storing them securely, and restoring them when needed.

Backup documentation is particularly useful for:
- Protecting against accidental data loss or corruption.
- Ensuring business continuity in case of hardware or software failures.
- Simplifying the process of restoring data during disaster recovery.
- Providing a clear plan for database administrators and developers to follow.

There are different types of backups and methods for documenting them, which will be explained in the following sections.

---

# Types of Backups

Backups can be categorized into different types based on their purpose and the data they include:

1. **Full Backup**:
   - A complete copy of the entire database, including all tables, data, and schema.
   - Useful for creating a baseline backup or restoring the database to a specific point in time.

2. **Incremental Backup**:
   - Captures only the changes made since the last backup.
   - Reduces storage requirements and speeds up the backup process.

3. **Differential Backup**:
   - Captures all changes made since the last full backup.
   - Provides a balance between storage efficiency and recovery speed.

4. **Logical Backup**:
   - Exports the database schema and data in a format like SQL scripts (e.g., `pg_dump` for PostgreSQL or `mysqldump` for MySQL).
   - Useful for migrating data between systems or creating portable backups.

5. **Physical Backup**:
   - Copies the database files directly from the file system.
   - Useful for large databases or when exact replication is required.

---

# Benefits of Backup Documentation

**Why is backup documentation important?**

### Answer:
Backup documentation provides a structured and reliable way to manage database backups and recovery processes. It allows organizations to:
- Minimize downtime by having clear recovery procedures.
- Ensure data integrity and consistency during restoration.
- Protect against data loss caused by human error, hardware failure, or cyberattacks.
- Simplify compliance with data protection regulations by maintaining proper backup records.

Backup documentation acts as a safety net for the database, ensuring that critical data can be restored quickly and efficiently when needed.

<details>
  <summary>Example</summary>
  - Full backup: A complete snapshot of the database taken weekly.
  - Incremental backup: Daily backups of changes since the last full backup.
  - Logical backup: Exporting the database schema and data using `mysqldump` or `pg_dump`.
</details>

---

# Backup Methods

The following sections provide step-by-step instructions for creating and restoring backups using tools specific to the database being used:

1. **MySQL**: Use `mysqldump` to create logical backups of the database schema and data.
2. **MongoDB**: Use `mongodump` to create BSON files for backing up and restoring NoSQL databases.

These tools allow you to back up your database in a portable format and restore it when needed, ensuring data integrity and consistency.

Before proceeding, ensure that the tasks in `01._migrations.md` have been completed. This includes:
1. Setting up the MySQL database using the `docker-compose.yaml` file.
2. Running the migrations for both the Node.js (`01._node_knex`) and Python (`02._python_alembic`) projects to create the necessary tables and schema.

---

## Take a MySQL Dump


Once the database is running and the migrations have been applied, you can create a backup of the database using `mysqldump`.

Run the following command to create a backup of the database using the `root` user. Make sure you are in the root of the project directory (`17._Database_Migrations`) and have a folder named `dumps` to store the dump files (VERY IMPORTANT USE GITBASH):

```bash
$ docker exec -i mysql_system_integration_migrations_container mysqldump --default-character-set=utf8mb4 -u root -prootpassword mydatabase > dumps/mysqldump.sql
```

### What’s Happening:
1. **`docker exec -i`**: Runs the `mysqldump` command inside the running MySQL container.
2. **`mysqldump`**: Creates a logical backup of the database schema and data.
3. **`-u root -prootpassword`**: Uses the `root` user and its password to authenticate with the MySQL database.
4. **`mydatabase`**: Specifies the name of the database to back up.
5. **`> dumps/mysqldump.sql`**: Saves the dump file in the `dumps` folder.

### Expectations:
- The MySQL container (`mysql_system_integration_migrations_container`) must be running. Check with:
  ```bash
  docker ps
  ```
- The database (`mydatabase`) must exist and have the schema and data created by running the migrations in both the Node.js and Python projects.
- The `dumps` folder must exist in the project directory. Create it if it doesn’t:
  ```bash
  mkdir dumps
  ```

This command will create a `mysqldump.sql` file in the `dumps` folder, which can be used to restore the database later.

*What does the file contain?*

<details> 
  <summary>Answer</summary>

   *The dump file contains:*
   ---

   - **DDL** (Data Definition Language): Defines the database schema (e.g., `CREATE TABLE`).
   - **DML** (Data Manipulation Language): Inserts data into the tables (e.g., `INSERT`).
   - **DCL** (Data Control Language): Manages permissions and access control (if applicable).

   This ensures the database can be fully recreated, including its structure, data, and access settings.
</details>

---

## Manage the MySQL Database

This section explains how to manage the MySQL database, including stopping and restarting the container, resetting the database to a clean state, and restoring the database from a dump file.


### Stop and Restart the Container

To stop and restart the MySQL container:

1. Stop the container:
   ```bash
   docker-compose down
   ```

2. Restart the container:
   ```bash
   docker-compose up -d
   ```

3. Verify that the database and data are still intact:
   - Connect to the MySQL database:
     ```bash
     docker exec -it mysql_system_integration_migrations_container mysql -u root -prootpassword
     ```

   - List all databases:
     ```sql
     SHOW DATABASES;
     ```

   - Switch to the target database:
     ```sql
     USE mydatabase;
     ```

   - List all tables:
     ```sql
     SHOW TABLES;
     ```

   - Check the contents of a table (e.g., `users`):
     ```sql
     SELECT * FROM users;
     ```

#### Why Does the Data Persist?

The database persists because the data is stored in a Docker volume (`system_integration_db_migrations_data`). Docker volumes are designed to retain data even when the container is stopped or restarted, ensuring that your database remains intact across container restarts.


### Get a Clean Database

If you want to reset the database and start with a clean slate, you need to remove the Docker volume where the data is stored. This will delete all data in the database.

#### Steps to Get a Clean Database:

1. Stop the database container:
   ```bash
   docker-compose down
   ```

2. Remove the volume:
   ```bash
   docker volume rm 17_database_migrations_system_integration_db_migrations_data
   ```

3. Restart the database container:
   ```bash
   docker-compose up -d
   ```

4. [Optional] Verify that the database is empty by connecting to it and listing the tables:
   ```bash
   docker exec -it mysql_system_integration_migrations_container mysql -u root -prootpassword
   ```
   ```sql
   SHOW DATABASES;
   USE mydatabase;
   SHOW TABLES;
   ```


### Load from the Dump into the Database

To restore the database from a previously created dump file:

1. Ensure the MySQL container is running:
   ```bash
   docker ps
   ```

2. Load the dump file into the database (Use PowerShell for this command):
   ```powershell
   Get-Content -Raw dumps/mysqldump.sql | docker exec -i mysql_system_integration_migrations_container mysql -u root -prootpassword mydatabase
   ```
   ```bash
   $ cat dumps/mysqldump.sql | docker exec -i mysql_system_integration_migrations_container mysql -u root -prootpassword mydatabase
   ```



### Verify the Restored Database

After loading the dump file, follow these steps to verify that the database has been restored correctly:

1. **Connect to the MySQL Database**:
   ```bash
   docker exec -it mysql_system_integration_migrations_container mysql -u root -prootpassword
   ```

2. **List All Databases**:
   ```sql
   SHOW DATABASES;
   ```

3. **Switch to the Target Database**:
   ```sql
   USE mydatabase;
   ```

4. **List All Tables**:
   ```sql
   SHOW TABLES;
   ```

5. **Check the Contents of a Table**:
   For example, to check the `users` table:
   ```sql
   SELECT * FROM users;
   ```

6. **Check Another Table**:
   For example, to check the `accounts` table:
   ```sql
   SELECT * FROM accounts;
   ```


---

# Load the File When Initializing the Database

To automatically load a dump file when initializing the MySQL database, follow these steps:

1. **Create an `initdb` Folder**:
   - In the same directory as the `docker-compose.yaml` file, create a folder named `initdb`:
     ```bash
     mkdir initdb
     ```

2. **Move the Dump File**:
   - Move the `mysqldump.sql` file to the `initdb` folder and rename it to `init.sql`:
     ```bash
     cp dumps/mysqldump.sql initdb/init.sql
     ```

3. **Update the `docker-compose.yaml` File**:
   - Modify the `docker-compose.yaml` file to mount the `init.sql` file as a volume. This ensures that MySQL automatically executes the SQL file during initialization:
     ```yaml
     services:
       db:
         image: mysql:latest
         container_name: mysql_system_integration_migrations_container
         env_file:
           - .env
         ports:
           - "${MYSQL_PORT}:3306"
         environment:
           - MYSQL_HOST=${MYSQL_HOST}
         volumes:
           - system_integration_db_migrations_data:/var/lib/mysql
           - ./initdb/init.sql:/docker-entrypoint-initdb.d/init.sql # new line

     volumes:
       system_integration_db_migrations_data:
     ```

4. **Recreate the Database**:
   - Take down the database and remove the volume to ensure a clean initialization:
     ```bash
     docker-compose down -v
     ```
   - Start the database again:
     ```bash
     docker-compose up -d
     ```

5. **Verify the Initialization**:
   - Connect to the MySQL container to verify that the database has been initialized with the data from the dump file:
     ```bash
     docker exec -it mysql_system_integration_migrations_container mysql -u root -prootpassword
     ```
   - Run the following SQL commands:
     ```sql
     SHOW DATABASES;
     USE mydatabase;
     SHOW TABLES;
     SELECT * FROM users;
     SELECT * FROM accounts;
     ```

---

# How to back up MongoDB

MongoDB creates BSON files which can be copied. If you paste them into the data folder then you will restore the data.

There is also a [mongodump](https://www.mongodb.com/docs/manual/tutorial/backup-and-restore-tools/) for MongoDB.

---

<div class="title-card">
    <h1>Documentation</h1>
</div>

---

# MRO: Generate Migrations from an Existing Schema

**MRO** stands for **Model Relations to Objects**. It is a tool that works in the opposite way of an ORM (Object-Relational Mapping). Instead of generating a database schema from code, MRO generates migration files or objects from an existing database schema.

This is useful when:
- You already have a database schema and want to create migration files for version control.
- You want to integrate an existing database into a project that uses migration tools like Knex.js.


## Why Use MRO?
- **Reverse Engineering**: It allows you to reverse-engineer an existing database schema into migration files, making it easier to manage schema changes in version control.
- **Integration**: It simplifies integrating legacy databases into modern projects that use tools like Knex.js for migrations.
- **Automation**: By using a `.env` file, you can streamline the process and avoid repetitive manual input.

## Steps to Generate Migrations

1. **Run from the Correct Directory**:
   Ensure you run the command from the directory where the `.env` file is located. This allows MRO to automatically pick up the database connection details (e.g., host, port, username, password, and database name) without requiring manual input.

2. **Automate Prompts (Optional)**:
   To skip prompts and automate the process, define the required keys in the `.env` file. For example:
   ```env
   DB_CLIENT=mysql2
   DB_HOST=localhost
   DB_PORT=3305
   DB_USER=myuser
   DB_PASSWORD=myuserpassword
   DB_NAME=mydatabase
   ```

   With these keys in place, MRO can automatically generate the migration files without requiring additional input. This is particularly useful for CI/CD pipelines or automated workflows.

3. **Run the MRO Command**:
   Use the following command to start the MRO tool:
   ```bash
   npx mro
   ```

4. **Choose the Prompts**:
   When running the MRO tool, you will be prompted to make several choices. Follow these steps:

   - **Choose Database**: Select `MySQL`.
   - **Choose an Output Format**: Select `Knex.js Migrations` to generate migration files compatible with Knex.js.
   - **Choose Module Syntax**: Select `ECMAScript Modules` if your project uses modern JavaScript (`import`/`export` syntax). Alternatively, select `CommonJS` if your project uses the traditional `require`/`module.exports` syntax.

## Apply the Generated Migration

The migration file (e.g., `20250411124949_mro_migration.js`) is a comprehensive script that can be used to apply schema changes to a MySQL database. It includes the schema definitions for both the Node.js project (`01._node_knex`) and the Python project (`02._python_alembic`), ensuring that the database is fully synchronized with the requirements of both.

### Key Points:
1. **Purpose**:
   - This migration file combines the schema changes from both projects, including tables like `accounts`, `alembic_version`, `products`, and `users`.
   - It ensures that the database is set up to support both the Node.js and Python workflows.

2. **Usage**:
   - To use this migration file, it must be placed in a directory where Knex is installed and configured (e.g., the `migrations` directory of the `01._node_knex` project).
   - Knex will then be able to execute the `up` and `down` functions defined in the file to apply or roll back the schema changes.

3. **Limitations**:
   - The migration file cannot be directly used from its current location (`17._Database_Migrations`) unless Knex is explicitly configured to include this directory in its `migrations.directory` setting.
   - Ensure that the environment where the migration is executed has access to the database and the necessary configuration (e.g., `.env` file).

4. **Integration**:
   - By running this migration, you can set up a unified database schema that supports both the Node.js and Python projects, streamlining development and ensuring consistency across the stack.

---

# MRO: Generate HTML Documentation

The MRO tool can also be used to generate HTML documentation for your database schema. This documentation provides a clear and visually structured overview of the database, including tables, columns, data types, and relationships.

1. Run the MRO command:
   ```bash
   npx mro
   ```

2. Follow the prompts:
   - **Choose Database**: Select `MySQL`.
   - **Choose Output Format**: Select `HTML Documentation`.
   - **Select Key-Value Pairs**: Choose the details to include for each column, such as `field` (column name), `type` (data type), `default` (default value), `null` (whether NULL is allowed), `key` (primary/foreign key), and `extra` (e.g., `AUTO_INCREMENT`).
   - **Select Tables**: Choose the tables you want to document (e.g., `accounts`, `products`, `users`).

3. The generated HTML file will include:
   - A list of all selected tables in the database.
   - Detailed information about each table, including the selected key-value pairs.
   - Relationships between tables, such as foreign key references.

This documentation is particularly useful for:
- Sharing the database structure with team members.
- Understanding the schema for debugging or development purposes.
- Including in project documentation for stakeholders.

---

# Generating Database Documentation

Database documentation can take many forms, depending on the type of database and the tools available. Here are some common approaches:

1. **Relational Databases**:
   - Use tools like MRO to generate HTML documentation or ER (Entity-Relationship) diagrams.
   - ER diagrams visually represent the tables, columns, and relationships in the database, making it easier to understand the schema at a glance.

2. **NoSQL Databases**:
   - Generate JSON schema documentation to describe the structure of collections and documents.
   - Use tools to create HTML-based documentation that outlines the fields, data types, and relationships in the NoSQL database.

3. **Automated Documentation Tools**:
   - Tools like `dbdocs.io`, `SchemaSpy`, or `DBeaver` can automate the process of generating detailed database documentation in various formats (HTML, PDF, etc.).

---

# Documenting Databases

Proper database documentation is essential for maintaining and understanding your database over time. Here are some best practices:

1. **Relational Databases**:
   - Use ER diagrams to visualize the schema and relationships between tables.
   - Include descriptions for tables, columns, and constraints directly in the database or in external documentation.

2. **NoSQL Databases**:
   - Document the structure of collections and documents using JSON schema.
   - Provide examples of typical documents to illustrate the expected data format.

3. **Version-Controlled Documentation**:
   - Store database documentation in version control (e.g., Git) alongside your codebase to ensure it stays up to date with schema changes.

4. **Integration with CI/CD**:
   - Automate the generation of database documentation as part of your CI/CD pipeline to ensure it reflects the latest schema changes.

By following these practices, you can ensure that your database is well-documented, making it easier for developers, database administrators, and stakeholders to work with and understand the system.